---
title: "forward-selection"
output: pdf_document
---
some problems in real life may have too much potential factors affect the responses we're interested,

for example, to model the accident rate on highway we might have predictors that describes the location of the highway, weather,
status of the road and the speed limits.

when we try to model the rate of having lung cancer, we might consider the predictors of air quality, frequency of smoking cigarette, genetic issue

includes all predictors will reduce the precision of our model, but ignore too much predictors might also makes the result of model untrustworthy,
so selecting appropriate predictors is critical for setting up accurate model.


step wise selection is a method help us select predictors that should be included in model.

one of the variations of step wise method is forward selection.

the idea of forward selection is starting at model consist only intercept, then consider all model consisting one additional regressor 
. compute the AIC score of each model and compare the AIC score of all models.

AIC = nlog(RSS/n) + 2p

RSS mean residual sum of square

p stands for the number of regressors

we keep the model that has the lowest AIC score, and repeat the previous step again, until there is no more
additional regressor or our current model has the lowest AIC score.




we use Robey.txt as example
```{r}
rob = read.table("Robey.txt")
summary(rob)
```


we use tfr as response, start at m1, which has only intercept

```{r cars}
m1 = lm(tfr ~ 1, data = rob)##current model
m2 = lm(tfr ~ 1 + contraceptors, data = rob)##intercept plus additional predictors contraceptors
m3 = lm(tfr ~ 1 + region, data = rob)##intercept plus additioanl predictors region
```

calculate the AIC value of each model,



```{r}
AIC(m1, m2, m3)
```

we keep m2, and repeat the previous step, since we have additional predictor region

```{r}
m4 = lm(tfr ~ 1 + contraceptors + region, data = rob)
AIC(m2, m4)
```
so according to forward selection, the best model should be m2


##the code 
```{r}
library(dplyr)
onestep_forward_selection = function(respons_e, star_t, en_d) {
  original = star_t
  mstart = lm(respons_e ~ ., data = star_t)#respons_e and star_t should only contain 1 element
  AIC_score = rep(NA, dim(en_d)[2])
  variable_name = rep(NA, dim(en_d)[2])
  AIC_mstart = AIC(mstart)
  for (i in 1:dim(en_d)[2]) {
    star_t$new_data = en_d[, i]
    names(star_t)[dim(star_t)[2]] = colnames(en_d[i])
    mtest = lm(respons_e ~., data = star_t)
    AIC_score[i] = AIC(mtest)
  }
  if (min(AIC_score) <= AIC_mstart) {
    index = which.min(AIC_score)
    star_t$new_data = subset(en_d, select = c(index))
    return(list(star_t, "go"))
  }
  else {
    return(list(original, "stop"))
  }
}

selection_function = function(respons, begin_data, full_data) {
  dro = colnames(begin_data)
  en_dd = full_data %>%
    select(-one_of(dro))
  sta_t = begin_data
  while(TRUE) {
    result = onestep_forward_selection(respons_e = respons, star_t = sta_t, en_d = en_dd)
    if (result[[2]] == "go") {
      sta_t = result[[1]]
      dropp = colnames(sta_t)
      en_dd = full_data %>%
        select(-one_of(dropp))
      if (dim(en_dd) == 0) {
        return(sta_t)
        break
      }
    }
    else if (result[[2]] == "stop") {
      return(sta_t)
      break
    }
  }
}
rob$region = as.factor(rob$region)
star_t = subset(rob, select = c(2, 3))
respons_e = unlist(subset(rob, select = c(2)))
#en_d = subset(rob, select = c(1))
dat_a = rob
#a = onestep_forward_selection(respons_e, star_t, en_d)
selection_function(respons = respons_e, begin_data = star_t, full_data = dat_a)
```



